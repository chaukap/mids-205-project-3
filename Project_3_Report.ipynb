{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Report\n",
    "***Chandler Haukap and Hassan Saad***\n",
    "\n",
    "In this project, we simulated several different events that could occur in Minesweeper. The `main.py` file contains the code that:\n",
    "* Creates the playing board\n",
    "* Simulates the location of mines specific to a game session\n",
    "* Allows for events such as clicking on a cell (check), flagging a cell (flag), or seeing the solution of the board (solution)\n",
    "    * The flag and check events allow for metadata to be created depending on if the specific cell contains an underlying mine or is a \"safe spot\"\n",
    "    \n",
    "The game interface consists of four endpoints:\n",
    "* /home is where the user requests a new game. This endpoint returns a unique UUID session ID that the user must submit to the other endpoints.\n",
    "    * parameters: none\n",
    "    * returns: A session identifier\n",
    "* /flag is how the user flags a location as having a bomb. If the space is not a bomb, the user loses (at least, in our version of Minesweeper)\n",
    "    * parameters: x, y, session_id\n",
    "    * returns: \"Correct\" if the space is a bomb, or \"Incorrect! Game over.\" if the space is not a bomb\n",
    "* /check is how the user tests a space. If the space doesn't have a bomb, they are safe. If it does have a bomb, they lose.\n",
    "    * parameters: x, y, session_id\n",
    "    * returns: \"Correct\" if the space is not a bomb, or \"Incorrect! Game over.\" if the space is a bomb\n",
    "* /solution If the user wants to give up they can get the solution via this endpoint. It returns an n x n grid representing the game board.\n",
    "    * parameter: session_id\n",
    "    * returns: The game board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Data Pipeline\n",
    "\n",
    "* Before we were able to do anything, we had to modify an existing `docker-compose.yml` file and updated the \"mids\" image so that we can have the latest version of Redis running in our Minesweeper API. You can view the image [on DockerHub](https://hub.docker.com/layers/180930452/hassansaadca/saad_project3/latest/images/sha256-e44f736674fb069e48aa8c2d1ecf072e5e5be26cfa2d1948b0284f59ddc5c6c2?context=repo).\n",
    "    \n",
    "    Now we're ready to run the API and store/access the data.\n",
    "    \n",
    "* First, the data gets created by the the Flask API, i.e. our `main.py` file, with the help of a python file that simulates gameplay (`event_generator.py`). With each event described above, data is generated in string format and fed into a Kafka queue. \n",
    "* Next, there are 3 ways in which we can log this data to a parquet file within the Hadoop environment. We explain these processes below, but for a summary:\n",
    "    1. We generate the data, then open a Jupyter notebook within the pyspark environment, and we use Pyspark to create the parquet file.\n",
    "    2. We generate the data, then run a Python file within the pyspark environment which takes a batch of data from the Kafka queue and writes it to a new parquet file.\n",
    "    3. We run a python file that runs an infinite loop and has the ability to continuously stream data to the Hadoop environment ever few seconds. As we run the `generate_events.py` file, this data gets automatically fed to the same parquet file.\n",
    "* Finally, we query the data within the Pyspark notebook (below)\n",
    "\n",
    "Below is a diagram of how our pipeline works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"205_p3_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup of the Data Pipeline and Getting the Flask API Running\n",
    "\n",
    "We do all of this within the working directory/ repository within our GCP VM.\n",
    "\n",
    "`~w205/mids-205-project-3/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spin up Docker Container:**\n",
    "\n",
    "`docker-compose up -d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start the flask API** \n",
    "\n",
    "`docker-compose exec mids env FLASK_APP=/w205/mids-205-project-3/main.py flask run --host 0.0.0.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a new shell terminal, **Create a Kafka Topic called 'events':**\n",
    "\n",
    "`docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --bootstrap-server kafka:29092`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a new shell terminal, **set up to watch the incoming Kafka Queue:** Remember to navigate to the working directory first.\n",
    "\n",
    "`docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Different ways of accessing and storing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using the Jupyter Notebook in the Pyspark Environment\n",
    "\n",
    "In this section, we'll use the most basic event in our API, the `solution` event. This is a situation in which a player checks to see where the mines are on the board. We'll filter for this event and write all the data points to a parquet file within the Jupyter notebook environment.\n",
    "\n",
    "After we've set up Kafka to watch the incoming queue, we can **run the command to generate data**\n",
    "\n",
    "`docker-compose exec mids python /w205/mids-205-project-3/generate_events.py`\n",
    "\n",
    "This will simulate 5 game sessions, and within each game the user \"peeks\" at the solution 5 times (so we get a total of 25 data points).\n",
    "\n",
    "Next, we have to access the Jupyter notebook after making the data from Kafka available within the same environment.\n",
    "\n",
    "We create a symbolic link to our working directory inside the Pyspark environment:\n",
    "\n",
    "`docker-compose exec spark bash`\n",
    "\n",
    "`ln -s /w205/mids-205-project-3 project3data`\n",
    "\n",
    "`exit`\n",
    "\n",
    "Then we launch a jupyter notebook, again inside the Pyspark container, using our VM external IP address:\n",
    "\n",
    "`docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa2446985f8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "| key|               value| topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     0|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     1|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     2|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     3|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     4|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     5|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     6|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     7|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     8|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     9|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    10|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    11|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    12|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    13|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    14|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    15|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    16|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    17|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    18|2021-12-08 21:57:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    19|2021-12-08 21:57:...|            0|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we can check to make sure that our data is of the correct size. For each session we generate 1 visit to the home page, and 100 to the solution, check, and flag pages totalling 301 events per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total events created: 1505\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of total events created: {raw_events.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_events = raw_events.select(raw_events.value.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a data point that was fed to the Kafka queue via the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event_type': 'solution',\n",
       " 'session_id': 'b9e269a3-644a-4c3e-84a8-8e2b641f0537'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(all_events.collect()[-1].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that we will use to filter for events of a specific type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "name = 'solution'\n",
    "\n",
    "@udf('boolean')\n",
    "def test(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == name:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data points from the raw events, extracting only the solution_events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "solution_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('stats'),\\\n",
    "            raw_events.timestamp.cast('string'))\\\n",
    "    .filter(test('stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "extracted_solution_events = solution_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.stats))) \\\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|event_type|          session_id|           timestamp|\n",
      "+----------+--------------------+--------------------+\n",
      "|  solution|e4400ca4-67d9-406...|2021-12-08 21:57:...|\n",
      "|  solution|e4400ca4-67d9-406...|2021-12-08 21:57:...|\n",
      "|  solution|e4400ca4-67d9-406...|2021-12-08 21:57:...|\n",
      "|  solution|e4400ca4-67d9-406...|2021-12-08 21:57:...|\n",
      "|  solution|e4400ca4-67d9-406...|2021-12-08 21:57:...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_solution_events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write this table to a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "extracted_solution_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/solution_requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the same Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "solution_batch = spark.read.parquet('/tmp/solution_requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table that we can use to run PySpark SQL commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "solution_batch.registerTempTable('solution_requests_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_type                            session_id                timestamp\n",
       "0   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.509\n",
       "1   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.524\n",
       "2   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.538\n",
       "3   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.554\n",
       "4   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.568"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions = spark.sql(\"select * from solution_requests_table\").toPandas()\n",
    "solutions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "How many different sessions were started? We can run a pandas command to just check for unique values of session_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Sessions: 5\n",
      "\n",
      "Unique Sessions:\n",
      "be0e3368-fdef-4861-9800-81c2e2183598\n",
      "36412095-d22e-4908-b16f-219f6e48a473\n",
      "31034adb-8130-44f7-bc1c-d8cdcd41b567\n",
      "0700aed7-a236-4855-b729-24a21964ee14\n",
      "191d876c-1f14-4deb-bfb8-b469433059fd\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Unique Sessions: {solutions.session_id.unique().shape[0]}\\n')\n",
    "print(f'Unique Sessions:')\n",
    "for i in solutions.session_id.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Method 2: Using a python file within the pyspark environment\n",
    "\n",
    "In this section, we'll use the other events in our API, the `flag` and `check` events. This `flag` event is a situation in which a player thinks there is a mine under a cell, so they flag it (usually with a right-click) as a warning and avoid uncovering it. The `check` event is a situation in which a player uncovers (left-clicks) a cell. If a mine is underneath the cell, the player loses, and if the cell is not a mine, the number of mines surrounding that cell is displayed.\n",
    "\n",
    "We can use the same data that we had generated before. Recall this was done by running this command above while using method 1:\n",
    "\n",
    "`docker-compose exec mids python /w205/mids-205-project-3/generate_events.py`\n",
    "\n",
    "Again, this simulated 5 different game sessions, and in each game, the user marked 60 potentially dangerous cells and uncovered 50 cells. We expect to see 500 `flag` events and 500 `check` events in our data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having generated the data, we run the `write_flag_events_batch.py` and `write_check_events_batch.py` files while inside the Pyspark environment. If we've already used Method 1, we don't need to create a symbolic link to our working directory again.\n",
    "\n",
    "`docker-compose exec spark bash`\n",
    "\n",
    "`cd project3data`\n",
    "\n",
    "`python write_flag_events_batch.py`\n",
    "\n",
    "This will create a parquet file for each event within our Hadoop environment: `/tmp/flag_cell` and `/tmp/check_cell`\n",
    "\n",
    "We can exit from the spark container shell and check the Hadoop environment file system with the following command:\n",
    "\n",
    "`docker-compose exec cloudera hadoop fs -ls /tmp/` \n",
    "\n",
    "...which yields the previous `solution_request` parquet file, the new `flag_cell` file and `check_cell` files as well.\n",
    "\n",
    "Now we can load these parquet files and query them with PySpark and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flag_batch = spark.read.parquet('/tmp/flag_cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flag_batch.registerTempTable('flag_cell_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>outcome</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:33.568</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:33.577</td>\n",
       "      <td>36</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:33.588</td>\n",
       "      <td>99</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:33.597</td>\n",
       "      <td>73</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:33.607</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_type    outcome                            session_id  \\\n",
       "0       flag  incorrect  e4400ca4-67d9-406d-abb6-a4e68c81e472   \n",
       "1       flag  incorrect  e4400ca4-67d9-406d-abb6-a4e68c81e472   \n",
       "2       flag  incorrect  e4400ca4-67d9-406d-abb6-a4e68c81e472   \n",
       "3       flag  incorrect  e4400ca4-67d9-406d-abb6-a4e68c81e472   \n",
       "4       flag  incorrect  e4400ca4-67d9-406d-abb6-a4e68c81e472   \n",
       "\n",
       "                 timestamp  x_coord  y_coord  \n",
       "0  2021-12-08 21:57:33.568        0       53  \n",
       "1  2021-12-08 21:57:33.577       36       98  \n",
       "2  2021-12-08 21:57:33.588       99       47  \n",
       "3  2021-12-08 21:57:33.597       73       37  \n",
       "4  2021-12-08 21:57:33.607       46       40  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags = spark.sql(\"select * from flag_cell_table\").toPandas()\n",
    "flags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that we did indeed write all 500 flag events in the Kafka Queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flag events logged in Parquet file: 500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of flag events logged in Parquet file: {flags.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what percentage of flags are put in appropriate spots and how many were placed in incorrect locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "incorrect_percentage = 100* flags[flags.outcome == 'incorrect'].shape[0]/flags.shape[0]\n",
    "correct_percentage = 100* flags[flags.outcome == 'correct'].shape[0]/flags.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Correct Flags: 17.05\n",
      "Percentage of Incorrect Flags: 83.0%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of Correct Flags: {correct_percentage}5')\n",
    "print(f'Percentage of Incorrect Flags: {incorrect_percentage}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the same distribution for each session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id                            outcome  \n",
       "0700aed7-a236-4855-b729-24a21964ee14  incorrect    83.333333\n",
       "                                      correct      16.666667\n",
       "191d876c-1f14-4deb-bfb8-b469433059fd  incorrect    86.666667\n",
       "                                      correct      13.333333\n",
       "31034adb-8130-44f7-bc1c-d8cdcd41b567  incorrect    81.666667\n",
       "                                      correct      18.333333\n",
       "36412095-d22e-4908-b16f-219f6e48a473  incorrect    80.000000\n",
       "                                      correct      20.000000\n",
       "be0e3368-fdef-4861-9800-81c2e2183598  incorrect    83.333333\n",
       "                                      correct      16.666667\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*flags.groupby('session_id').outcome.value_counts()/flags.groupby('session_id').outcome.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a similar process with the check functions. We'll query to see what percentage of uncovered cells had underlying mindes and what cells were safe (aggregated by session ID). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_batch = spark.read.parquet('/tmp/check_cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_batch.registerTempTable('check_cell_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>neighboring_bombs</th>\n",
       "      <th>outcome</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>check</td>\n",
       "      <td>26</td>\n",
       "      <td>safe</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:31.983</td>\n",
       "      <td>66</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check</td>\n",
       "      <td>62</td>\n",
       "      <td>safe</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:31.999</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>check</td>\n",
       "      <td>12</td>\n",
       "      <td>safe</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:32.022</td>\n",
       "      <td>84</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check</td>\n",
       "      <td>11</td>\n",
       "      <td>hit_mine</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:32.041</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check</td>\n",
       "      <td>43</td>\n",
       "      <td>safe</td>\n",
       "      <td>e4400ca4-67d9-406d-abb6-a4e68c81e472</td>\n",
       "      <td>2021-12-08 21:57:32.061</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_type  neighboring_bombs   outcome  \\\n",
       "0      check                 26      safe   \n",
       "1      check                 62      safe   \n",
       "2      check                 12      safe   \n",
       "3      check                 11  hit_mine   \n",
       "4      check                 43      safe   \n",
       "\n",
       "                             session_id                timestamp  x_coord  \\\n",
       "0  e4400ca4-67d9-406d-abb6-a4e68c81e472  2021-12-08 21:57:31.983       66   \n",
       "1  e4400ca4-67d9-406d-abb6-a4e68c81e472  2021-12-08 21:57:31.999        1   \n",
       "2  e4400ca4-67d9-406d-abb6-a4e68c81e472  2021-12-08 21:57:32.022       84   \n",
       "3  e4400ca4-67d9-406d-abb6-a4e68c81e472  2021-12-08 21:57:32.041       82   \n",
       "4  e4400ca4-67d9-406d-abb6-a4e68c81e472  2021-12-08 21:57:32.061       42   \n",
       "\n",
       "   y_coord  \n",
       "0       82  \n",
       "1       66  \n",
       "2       95  \n",
       "3       14  \n",
       "4       40  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checks = spark.sql(\"select * from check_cell_table\").toPandas()\n",
    "checks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that we have 500 check events (100 per each of the 5 sessions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of check events logged in Parquet file: 500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of check events logged in Parquet file: {checks.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id                            outcome \n",
       "17ba600a-0ac8-42df-94af-14fd53d84222  safe        85.0\n",
       "                                      hit_mine    15.0\n",
       "1d9c8840-6e20-49a9-8928-0f9b13de5be2  safe        81.0\n",
       "                                      hit_mine    19.0\n",
       "274f89df-d1c5-4c20-b338-1be0813a5ee3  safe        82.0\n",
       "                                      hit_mine    18.0\n",
       "b9e269a3-644a-4c3e-84a8-8e2b641f0537  safe        79.0\n",
       "                                      hit_mine    21.0\n",
       "e4400ca4-67d9-406d-abb6-a4e68c81e472  safe        79.0\n",
       "                                      hit_mine    21.0\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*checks.groupby('session_id').outcome.value_counts()/checks.groupby('session_id').outcome.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Method 3: Writing Data to parquet files in a streaming manner\n",
    "\n",
    "In this section, we'll use the last event in our API( the `check` event) again, just to demonstrate how live streaming of events work. \n",
    "\n",
    "This time, we run a python file within the spark environment in preparation to read incoming data from the Kafka queue and write it to a parquet file.\n",
    "\n",
    "We enter the spark container shell... \n",
    "\n",
    "`docker-compose exec spark bash`\n",
    "\n",
    "...and run the following commands:\n",
    "\n",
    "`cd project3data`\n",
    "\n",
    "`python write_check_events_stream.py`\n",
    "\n",
    "Note that we run this file before we start producing the data. This runs until interrupted manually. \n",
    "\n",
    "*This is in comparison method 2, for which we generated data, then we wrote it in batches to parquet files.*\n",
    "\n",
    "After this, we test to see if it works by now calling the `generate_events.py` file from within our original working directory (i.e. `~/w205/mids-205-project-3`). \n",
    "\n",
    "Once we run this data generating code, `write_check_events_stream.py` adds to a parquet file to the Hadoop environment as data comes into the Kafka queue, which we could then load as a PySpark SQL table by running the commands below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_batch = spark.read.parquet('/tmp/check_stream_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_batch.registerTempTable('flag_cell_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, in the same manner as we've done above, the flag_cell_table is available for querying through PySpark SQL."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
