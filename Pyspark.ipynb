{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Report\n",
    "***Chandler Haukap and Hassan Saad***\n",
    "\n",
    "In this project, we simulated several different events that could occur in Minesweeper. The `main.py` file contains the code that:\n",
    "* Creates the playing board\n",
    "* Simulates the location of mines specific to a game session\n",
    "* Allows for events such as clicking on a cell (check), flagging a cell (flag), or seeing the solution of the board (solution)\n",
    "    * The flag and check events allow for metadata to be created depending on if the specific cell contains an underlying mine or is a \"safe spot\"\n",
    "    \n",
    "The game interface consists of four endpoints:\n",
    "* /home is where the user requests a new game. This endpoint returns a unique UUID session ID that the user must submit to the other endpoints.\n",
    "    * parameters: none\n",
    "    * returns: A session identifier\n",
    "* /flag is how the user flags a location as having a bomb. If the space is not a bomb, the user loses (at least, in our version of Minesweeper)\n",
    "    * parameters: x, y, session_id\n",
    "    * returns: \"Correct\" if the space is a bomb, or \"Incorrect! Game over.\" if the space is not a bomb\n",
    "* /check is how the user tests a space. If the space doesn't have a bomb, they are safe. If it does have a bomb, they lose.\n",
    "    * parameters: x, y, session_id\n",
    "    * returns: \"Correct\" if the space is not a bomb, or \"Incorrect! Game over.\" if the space is a bomb\n",
    "* /solution If the user wants to give up they can get the solution via this endpoint. It returns an n x n grid representing the game board.\n",
    "    * parameter: session_id\n",
    "    * returns: The game board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Data Pipeline\n",
    "\n",
    "* Before we were able to do anything, we had to modify an existing `docker-compose.yml` file and updated the \"mids\" image so that we can have the latest version of Redis running in our Minesweeper API. You can view the image [on DockerHub](https://hub.docker.com/layers/180930452/hassansaadca/saad_project3/latest/images/sha256-e44f736674fb069e48aa8c2d1ecf072e5e5be26cfa2d1948b0284f59ddc5c6c2?context=repo).\n",
    "    \n",
    "    Now we're ready to run the API and store/access the data.\n",
    "    \n",
    "* First, the data gets created by the the Flask API, i.e. our `main.py` file, with the help of a python file that simulates gameplay (`event_generator.py`). With each event described above, data is generated in string format and fed into a Kafka queue. \n",
    "* Next, there are 3 ways in which we can log this data to a parquet file within the Hadoop environment. We explain these processes below, but for a summary:\n",
    "    1. We generate the data, then open a Jupyter notebook within the pyspark environment, and we use Pyspark to create the parquet file.\n",
    "    2. We generate the data, then run a Python file within the pyspark environment which takes a batch of data from the Kafka queue and writes it to a new parquet file.\n",
    "    3. We run a python file that runs an infinite loop and has the ability to continuously stream data to the Hadoop environment ever few seconds. As we run the `generate_events.py` file, this data gets automatically fed to the same parquet file.\n",
    "* Finally, we query the data within the Pyspark notebook (below)\n",
    "\n",
    "Below is a diagram of how our pipeline works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"205_p3_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup of the Data Pipeline and Getting the Flask API Running\n",
    "\n",
    "We do all of this within the working directory/ repository within our GCP VM.\n",
    "\n",
    "`~w205/mids-205-project-3/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spin up Docker Container:**\n",
    "\n",
    "`docker-compose up -d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Kafka Topic, in this case called events:**\n",
    "\n",
    "`docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --bootstrap-server kafka:29092`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a new shell terminal, **set up to watch the incoming Kafka Queue:** Remember to navigate to the working directory first.\n",
    "\n",
    "`docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Different ways of accessing and storing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using the Jupyter Notebook in the Pyspark Environment\n",
    "\n",
    "In this section, we'll use the most basic event in our API, the `solution` event. This is a situation in which a player checks to see where the mines are on the board. We'll filter for this event and write all the data points to a parquet file within the Jupyter notebook environment.\n",
    "\n",
    "After we've set up Kafka to watch the incoming queue, we can **run the command to generate data**\n",
    "\n",
    "`docker-compose exec mids python /w205/mids-205-project-3/generate_events.py`\n",
    "\n",
    "This will simulate 5 game sessions, and within each game the user \"peeks\" at the solution 5 times (so we get a total of 25 data points).\n",
    "\n",
    "Next, we have to access the Jupyter notebook after making the data from Kafka available within the same environment.\n",
    "\n",
    "We create a symbolic link to our working directory inside the Pyspark environment:\n",
    "\n",
    "`docker-compose exec spark bash`\n",
    "\n",
    "`ln -s /w205/mids-205-project-3 project3data`\n",
    "\n",
    "`exit`\n",
    "\n",
    "Then we launch a jupyter notebook, again inside the Pyspark container, using our VM external IP address:\n",
    "\n",
    "`docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa5236d25f8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "| key|               value| topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     0|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     1|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     2|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     3|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     4|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     5|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     6|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     7|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     8|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|     9|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    10|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    11|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    12|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    13|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    14|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    15|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    16|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    17|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    18|2021-12-07 23:42:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    19|2021-12-07 23:42:...|            0|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_events = raw_events.select(raw_events.value.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a data point that was fed to the Kafka queue via the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event_type': 'solution',\n",
       " 'session_id': '191d876c-1f14-4deb-bfb8-b469433059fd'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(all_events.collect()[-1].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that we will use to filter for events of a specific type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "name = 'solution'\n",
    "\n",
    "@udf('boolean')\n",
    "def test(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == name:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data points from the raw events, extracting only the solution_events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "solution_events = raw_events \\\n",
    "    .select(raw_events.value.cast('string').alias('stats'),\\\n",
    "            raw_events.timestamp.cast('string'))\\\n",
    "    .filter(test('stats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "extracted_solution_events = solution_events \\\n",
    "    .rdd \\\n",
    "    .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.stats))) \\\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|event_type|          session_id|           timestamp|\n",
      "+----------+--------------------+--------------------+\n",
      "|  solution|be0e3368-fdef-486...|2021-12-07 23:42:...|\n",
      "|  solution|be0e3368-fdef-486...|2021-12-07 23:42:...|\n",
      "|  solution|be0e3368-fdef-486...|2021-12-07 23:42:...|\n",
      "|  solution|be0e3368-fdef-486...|2021-12-07 23:42:...|\n",
      "|  solution|be0e3368-fdef-486...|2021-12-07 23:42:...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_solution_events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write this table to a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "extracted_solution_events \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('/tmp/solution_requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the same Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "solution_batch = spark.read.parquet('/tmp/solution_requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table that we can use to run PySpark SQL commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "solution_batch.registerTempTable('solution_requests_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>solution</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:20.568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_type                            session_id                timestamp\n",
       "0   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.509\n",
       "1   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.524\n",
       "2   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.538\n",
       "3   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.554\n",
       "4   solution  be0e3368-fdef-4861-9800-81c2e2183598  2021-12-07 23:42:20.568"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions = spark.sql(\"select * from solution_requests_table\").toPandas()\n",
    "solutions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "How many different sessions were started? We can run a pandas command to just check for unique values of session_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Sessions: 5\n",
      "\n",
      "Unique Sessions:\n",
      "be0e3368-fdef-4861-9800-81c2e2183598\n",
      "36412095-d22e-4908-b16f-219f6e48a473\n",
      "31034adb-8130-44f7-bc1c-d8cdcd41b567\n",
      "0700aed7-a236-4855-b729-24a21964ee14\n",
      "191d876c-1f14-4deb-bfb8-b469433059fd\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Unique Sessions: {solutions.session_id.unique().shape[0]}\\n')\n",
    "print(f'Unique Sessions:')\n",
    "for i in solutions.session_id.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Method 2: Using a python file within the pyspark environment\n",
    "\n",
    "In this section, we'll use another event in our API, the `flag` event. This is a situation in which a player thinks there is a mine under a cell, so they flag it as a warning and avoid uncovering it.\n",
    "\n",
    "We can use the same data that we had generated before, after we ran the command:\n",
    "\n",
    "`docker-compose exec mids python /w205/mids-205-project-3/generate_events.py`\n",
    "\n",
    "Again, this simulated 5 different game sessions, and in each game, the user marked 60 potentially dangerous cells. We expect to see 300 flag events in our data set.\n",
    "\n",
    "\n",
    "5*(60+50+5+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having generated the data, we run the `write_flag_events_batch.py` file while inside the Pyspark environment. If we've already used Method 1, we don't need to create a symbolic link to our working directory again.\n",
    "\n",
    "`docker-compose exec spark bash`\n",
    "\n",
    "`cd project3data`\n",
    "\n",
    "`python write_flag_events_batch.py`\n",
    "\n",
    "This will create a parquet file within our Hadoop environment `/tmp/flag_cell`\n",
    "\n",
    "We can exit from the spark container shell and check the Hadoop environment file system with the following command:\n",
    "\n",
    "`docker-compose exec cloudera hadoop fs -ls /tmp/` \n",
    "\n",
    "Which yields the previous solution_request parquet file and the new flag_cell file as well.\n",
    "\n",
    "Now we can load this parquet file and query it with PySpark and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flag_batch = spark.read.parquet('/tmp/flag_cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flag_batch.registerTempTable('flag_cell_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_type</th>\n",
       "      <th>outcome</th>\n",
       "      <th>session_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:19.952</td>\n",
       "      <td>80</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:19.961</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flag</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:19.969</td>\n",
       "      <td>59</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flag</td>\n",
       "      <td>correct</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:19.984</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flag</td>\n",
       "      <td>correct</td>\n",
       "      <td>be0e3368-fdef-4861-9800-81c2e2183598</td>\n",
       "      <td>2021-12-07 23:42:19.997</td>\n",
       "      <td>96</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  event_type    outcome                            session_id  \\\n",
       "0       flag  incorrect  be0e3368-fdef-4861-9800-81c2e2183598   \n",
       "1       flag  incorrect  be0e3368-fdef-4861-9800-81c2e2183598   \n",
       "2       flag  incorrect  be0e3368-fdef-4861-9800-81c2e2183598   \n",
       "3       flag    correct  be0e3368-fdef-4861-9800-81c2e2183598   \n",
       "4       flag    correct  be0e3368-fdef-4861-9800-81c2e2183598   \n",
       "\n",
       "                 timestamp  x_coord  y_coord  \n",
       "0  2021-12-07 23:42:19.952       80       61  \n",
       "1  2021-12-07 23:42:19.961       29       11  \n",
       "2  2021-12-07 23:42:19.969       59       86  \n",
       "3  2021-12-07 23:42:19.984       64        8  \n",
       "4  2021-12-07 23:42:19.997       96       87  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags = spark.sql(\"select * from flag_cell_table\").toPandas()\n",
    "flags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that we did indeed write all 300 flag events in the Kafka Queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flag events logged in Parquet file: 300\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of flag events logged in Parquet file: {flags.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what percentage of flags are put in appropriate spots and how many were placed in incorrect locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "incorrect_percentage = 100* flags[flags.outcome == 'incorrect'].shape[0]/flags.shape[0]\n",
    "correct_percentage = 100* flags[flags.outcome == 'correct'].shape[0]/flags.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Correct Flags: 17.05\n",
      "Percentage of Incorrect Flags: 83.0%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of Correct Flags: {correct_percentage}5')\n",
    "print(f'Percentage of Incorrect Flags: {incorrect_percentage}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the same distribution for each session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id                            outcome  \n",
       "0700aed7-a236-4855-b729-24a21964ee14  incorrect    83.333333\n",
       "                                      correct      16.666667\n",
       "191d876c-1f14-4deb-bfb8-b469433059fd  incorrect    86.666667\n",
       "                                      correct      13.333333\n",
       "31034adb-8130-44f7-bc1c-d8cdcd41b567  incorrect    81.666667\n",
       "                                      correct      18.333333\n",
       "36412095-d22e-4908-b16f-219f6e48a473  incorrect    80.000000\n",
       "                                      correct      20.000000\n",
       "be0e3368-fdef-4861-9800-81c2e2183598  incorrect    83.333333\n",
       "                                      correct      16.666667\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*flags.groupby('session_id').outcome.value_counts()/flags.groupby('session_id').outcome.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Method 3: Writing Data to parquet files in a streaming manner\n",
    "\n",
    "In this section, we'll the last event in our API, the `check` event. This is a situation in which a player uncovers (usually with a left click) a cell. If a mine is underneath the cell, the player loses, and if the cell is not a mine, the number of mines surrounding that cell is displayed.\n",
    "\n",
    "The same data remains in the kafka queue, consisting of (per each of the 5 sessions) 5 visits to the home page, 60 \"flag\" events, 50 \"check\" events, and 5 solution requests. The total event data points in the queue is therefore 580.\n",
    "\n",
    "This time, we run a python file within the spark environment in preparation to read incoming data from the Kafka queue and write it to a parquet file.\n",
    "\n",
    "We enter the spark container shell, and run the following commands:\n",
    "\n",
    "`docker-compose exec spark bash`\n",
    "\n",
    "`cd project3data`\n",
    "\n",
    "`python write_check_events_stream.py`\n",
    "\n",
    "This runs until interrupted manually.\n",
    "\n",
    "After this, we test to see if it works by now calling a python file from our original working directory. This time, the data generator has a timing function in it, so that it creates a data point every half second and there is a delay between sessions (5 seconds). \n",
    "\n",
    "\n",
    "\n",
    "Once we run this data generating code, `write_check_events_stream.py` adds to a parquet file to the Hadoop environment, which we then load as a PySpark SQL table below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_batch = spark.read.parquet('/tmp/check_stream_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_batch.registerTempTable('flag_cell_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
